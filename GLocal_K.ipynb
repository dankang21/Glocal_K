{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fleanend/TorchGlocalK/blob/main/Glocal_K.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Glocal_K"
      ],
      "metadata": {
        "id": "liOwWguk7GnL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ty3gYQgtnwFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ead2d8-0124-4745-cfb2-e0369ba2596e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Glocal_K'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 92 (delta 26), reused 14 (delta 4), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (92/92), 28.55 MiB | 10.82 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dankang21/Glocal_K.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "n14TfCE0X3JE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MovieLens-100K와 amazon data 선택"
      ],
      "metadata": {
        "id": "9vbtHW8NYCDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/Glocal_K/data/MovieLens_100K/u.data'\n",
        "r_cols = ['user_id', 'item_id', 'rating', 'timestamp']\n",
        "ratings = pd.read_csv(path, names=r_cols,  sep='\\t',encoding='latin-1')\n",
        "ratings = ratings[['user_id', 'item_id', 'rating']].astype(int)            # timestamp 제거\n"
      ],
      "metadata": {
        "id": "Ah9-91m_XXqB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/Glocal_K/data/amazon/Amazon_ratings.csv'\n",
        "ratings = pd.read_csv(path, encoding='latin-1')"
      ],
      "metadata": {
        "id": "DpTK9m-UkBPP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_idx = {}\n",
        "item_idx = {}\n",
        "for i in set(ratings.user_id):\n",
        "    user_idx[i] = len(user_idx)\n",
        "for i in set(ratings.item_id):\n",
        "    item_idx[i] = len(item_idx)\n",
        "\n",
        "M = len(user_idx)\n",
        "N = len(item_idx)"
      ],
      "metadata": {
        "id": "meDkFSM3kEcC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings['item_idx'] = ratings['item_id'].map(item_idx)\n",
        "ratings['user_idx'] = ratings['user_id'].map(user_idx)"
      ],
      "metadata": {
        "id": "ZPvnncOwkJgW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_ratings = ratings['rating'].unique()\n",
        "print(unique_ratings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBTCNa7OkQgE",
        "outputId": "4b33785d-7066-4dbb-a284-f9e4c3e4fe21"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 1 2 4 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = ratings[['user_idx', 'item_idx', 'rating']]"
      ],
      "metadata": {
        "id": "HckARYnbkVQl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test 분리\n",
        "from sklearn.utils import shuffle\n",
        "TRAIN_SIZE = 0.75\n",
        "ratings = shuffle(ratings, random_state=12)\n",
        "cutoff = int(TRAIN_SIZE * len(ratings))\n",
        "ratings_train = ratings.iloc[:cutoff]\n",
        "ratings_test = ratings.iloc[cutoff:]\n",
        "mu = ratings_train.rating.mean()    # 전체 평균"
      ],
      "metadata": {
        "id": "Vq1sxWaqkWH6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs8Ob5SiIC4u",
        "outputId": "eeea120c-a374-4dcf-fde1-9dae2f63e114"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.5278933333333335"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "ratings_train_arr = ratings_train.to_numpy(dtype=np.int32)\n",
        "ratings_test_arr = ratings_test.to_numpy(dtype=np.int32)"
      ],
      "metadata": {
        "id": "WKoLR5PPkYOR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nl2tU6kL8Ot3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b124a0a-e9ca-40e9-96bb-d49d6534bf0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "from time import time\n",
        "from scipy.sparse import csc_matrix\n",
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "torch.manual_seed(1284)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4A9uU1WloQ2"
      },
      "source": [
        "# Data Loader Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_amazon():\n",
        "    train = ratings_train_arr\n",
        "    test = ratings_test_arr\n",
        "    total = np.concatenate((train, test), axis=0)\n",
        "\n",
        "    n_u = np.unique(total[:,0]).size  # num of users\n",
        "    n_m = np.unique(total[:,1]).size  # num of items\n",
        "    n_train = train.shape[0]  # num of training ratings\n",
        "    n_test = test.shape[0]  # num of test ratings\n",
        "\n",
        "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "\n",
        "    for i in range(n_train):\n",
        "        train_r[train[i,1]-1, train[i,0]-1] = train[i,2]\n",
        "\n",
        "    for i in range(n_test):\n",
        "        test_r[test[i,1]-1, test[i,0]-1] = test[i,2]\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of items: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_train))\n",
        "    print('num of test ratings: {}'.format(n_test))\n",
        "\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m"
      ],
      "metadata": {
        "id": "7y6k4Ou5hNU1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8kEkg9mlIW"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJqSSY33mgkw",
        "outputId": "b9c055cf-a612-44c5-f7dc-380606da788f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data matrix loaded\n",
            "num of users: 943\n",
            "num of items: 1682\n",
            "num of training ratings: 75000\n",
            "num of test ratings: 25000\n"
          ]
        }
      ],
      "source": [
        "# Data Load\n",
        "try:\n",
        "    n_m, n_u, train_r, train_m, test_r, test_m = load_data_amazon()\n",
        "except Exception:\n",
        "    print('Error: Unable to load data')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "하이퍼파라미터"
      ],
      "metadata": {
        "id": "fStcPwusZW2F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nGCdp_FlobOK"
      },
      "outputs": [],
      "source": [
        "# Common hyperparameter settings\n",
        "n_hid = 500 # size of hidden layers\n",
        "n_dim = 5 # inner AE embedding size\n",
        "n_layers = 2 # number of hidden layers\n",
        "gk_size = 3 # width=height of kernel for convolution\n",
        "\n",
        "# Hyperparameters to tune for specific case\n",
        "max_epoch_p = 500 # max number of epochs for pretraining\n",
        "max_epoch_f = 1000 # max number of epochs for finetuning\n",
        "patience_p = 5 # number of consecutive rounds of early stopping condition before actual stop for pretraining\n",
        "patience_f = 10 # and finetuning\n",
        "tol_p = 1e-4 # minimum threshold for the difference between consecutive values of train rmse, used for early stopping, for pretraining\n",
        "tol_f = 1e-5 # and finetuning\n",
        "lambda_2 = 20. # regularisation of number or parameters\n",
        "lambda_s = 0.006 # regularisation of sparsity of the final matrix\n",
        "dot_scale = 1 # dot product weight for global kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sWtU4-pmDDT"
      },
      "source": [
        "# Network Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "p1P6fgYiy28F"
      },
      "outputs": [],
      "source": [
        "def local_kernel(u, v):\n",
        "    dist = torch.norm(u - v, p=2, dim=2)\n",
        "    hat = torch.clamp(1. - dist**2, min=0.)\n",
        "    return hat\n",
        "\n",
        "class KernelLayer(nn.Module):\n",
        "    def __init__(self, n_in, n_hid, n_dim, lambda_s, lambda_2, activation=nn.Sigmoid()):\n",
        "      super().__init__()\n",
        "      self.W = nn.Parameter(torch.randn(n_in, n_hid))\n",
        "      self.u = nn.Parameter(torch.randn(n_in, 1, n_dim))\n",
        "      self.v = nn.Parameter(torch.randn(1, n_hid, n_dim))\n",
        "      self.b = nn.Parameter(torch.randn(n_hid))\n",
        "\n",
        "      self.lambda_s = lambda_s\n",
        "      self.lambda_2 = lambda_2\n",
        "\n",
        "      nn.init.xavier_uniform_(self.W, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      nn.init.xavier_uniform_(self.u, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      nn.init.xavier_uniform_(self.v, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      nn.init.zeros_(self.b)\n",
        "      self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "      w_hat = local_kernel(self.u, self.v)\n",
        "\n",
        "      sparse_reg = torch.nn.functional.mse_loss(w_hat, torch.zeros_like(w_hat))\n",
        "      sparse_reg_term = self.lambda_s * sparse_reg\n",
        "\n",
        "      l2_reg = torch.nn.functional.mse_loss(self.W, torch.zeros_like(self.W))\n",
        "      l2_reg_term = self.lambda_2 * l2_reg\n",
        "\n",
        "      W_eff = self.W * w_hat  # Local kernelised weight matrix\n",
        "      y = torch.matmul(x, W_eff) + self.b\n",
        "      y = self.activation(y)\n",
        "\n",
        "      return y, sparse_reg_term + l2_reg_term\n",
        "\n",
        "class KernelNet(nn.Module):\n",
        "    def __init__(self, n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2):\n",
        "      super().__init__()\n",
        "      layers = []\n",
        "      for i in range(n_layers):\n",
        "        if i == 0:\n",
        "          layers.append(KernelLayer(n_u, n_hid, n_dim, lambda_s, lambda_2))\n",
        "        else:\n",
        "          layers.append(KernelLayer(n_hid, n_hid, n_dim, lambda_s, lambda_2))\n",
        "      layers.append(KernelLayer(n_hid, n_u, n_dim, lambda_s, lambda_2, activation=nn.Identity()))\n",
        "      self.layers = nn.ModuleList(layers)\n",
        "      self.dropout = nn.Dropout(0.33)\n",
        "\n",
        "    def forward(self, x):\n",
        "      total_reg = None\n",
        "      for i, layer in enumerate(self.layers):\n",
        "        x, reg = layer(x)\n",
        "        if i < len(self.layers)-1:\n",
        "          x = self.dropout(x)\n",
        "        if total_reg is None:\n",
        "          total_reg = reg\n",
        "        else:\n",
        "          total_reg += reg\n",
        "      return x, total_reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7RGKh1ckXgtP"
      },
      "outputs": [],
      "source": [
        "class CompleteNet(nn.Module):\n",
        "    def __init__(self, kernel_net, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size, dot_scale):\n",
        "      super().__init__()\n",
        "      self.gk_size = gk_size\n",
        "      self.dot_scale = dot_scale\n",
        "      self.local_kernel_net = kernel_net\n",
        "      self.conv_kernel = torch.nn.Parameter(torch.randn(n_m, gk_size**2) * 0.1)\n",
        "      nn.init.xavier_uniform_(self.conv_kernel, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "\n",
        "\n",
        "    def forward(self, x, x_local):\n",
        "      gk = self.global_kernel(x_local, self.gk_size, self.dot_scale)\n",
        "      x = self.global_conv(x, gk)\n",
        "      x, global_reg_loss = self.local_kernel_net(x)\n",
        "      return x, global_reg_loss\n",
        "\n",
        "    def global_kernel(self, input, gk_size, dot_scale):\n",
        "      avg_pooling = torch.mean(input, dim=1)  # Item (axis=1) based average pooling\n",
        "      avg_pooling = avg_pooling.view(1, -1)\n",
        "\n",
        "      gk = torch.matmul(avg_pooling, self.conv_kernel) * dot_scale  # Scaled dot product\n",
        "      gk = gk.view(1, 1, gk_size, gk_size)\n",
        "\n",
        "      return gk\n",
        "\n",
        "    def global_conv(self, input, W):\n",
        "      input = input.unsqueeze(0).unsqueeze(0)\n",
        "      conv2d = nn.LeakyReLU()(F.conv2d(input, W, stride=1, padding=1))\n",
        "      return conv2d.squeeze(0).squeeze(0)\n",
        "\n",
        "class Loss(nn.Module):\n",
        "    def forward(self, pred_p, reg_loss, train_m, train_r):\n",
        "      # L2 loss\n",
        "      diff = train_m * (train_r - pred_p)\n",
        "      sqE = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff))\n",
        "      loss_p = sqE + reg_loss\n",
        "      return loss_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8sQCwrSmKG4"
      },
      "source": [
        "# Network Instantiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOtWj1SCo1RW"
      },
      "source": [
        "## Pre-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7teUrgWagpW0"
      },
      "outputs": [],
      "source": [
        "model = KernelNet(n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2).double().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IEBsNhNo4Cj"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "OiTXqnN6zLXQ"
      },
      "outputs": [],
      "source": [
        "complete_model = CompleteNet(model, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size, dot_scale).double().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sETwz58aK6y6"
      },
      "source": [
        "# Evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vyReXxgac3KH"
      },
      "outputs": [],
      "source": [
        "def dcg_k(score_label, k):\n",
        "    dcg, i = 0., 0\n",
        "    for s in score_label:\n",
        "        if i < k:\n",
        "            dcg += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    return dcg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "jwsSR-8ZdGWo"
      },
      "outputs": [],
      "source": [
        "def ndcg_k(y_hat, y, k):\n",
        "    score_label = np.stack([y_hat, y], axis=1).tolist()\n",
        "    score_label = sorted(score_label, key=lambda d:d[0], reverse=True)\n",
        "    score_label_ = sorted(score_label, key=lambda d:d[1], reverse=True)\n",
        "    norm, i = 0., 0\n",
        "    for s in score_label_:\n",
        "        if i < k:\n",
        "            norm += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    dcg = dcg_k(score_label, k)\n",
        "    return dcg / norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "yy9eQS51pbhj"
      },
      "outputs": [],
      "source": [
        "def call_ndcg(y_hat, y):\n",
        "    ndcg_sum, num = 0, 0\n",
        "    y_hat, y = y_hat.T, y.T\n",
        "    n_users = y.shape[0]\n",
        "\n",
        "    for i in range(n_users):\n",
        "        y_hat_i = y_hat[i][np.where(y[i])]\n",
        "        y_i = y[i][np.where(y[i])]\n",
        "\n",
        "        if y_i.shape[0] < 2:\n",
        "            continue\n",
        "\n",
        "        ndcg_sum += ndcg_k(y_hat_i, y_i, y_i.shape[0])  # user-wise calculation\n",
        "        num += 1\n",
        "\n",
        "    return ndcg_sum / num"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXXQjeMxmYEC"
      },
      "source": [
        "# Training and Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ35Zoha-Eue",
        "outputId": "9c053f69-4e9e-440a-d80e-50c5639e1847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 0 test rmse: 2.7459853 train rmse: 2.7417574\n",
            "Time: 3.5531342029571533 seconds\n",
            "Time cumulative: 3.5531342029571533 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 29 test rmse: 1.1182712 train rmse: 1.1062294\n",
            "Time: 4.334450006484985 seconds\n",
            "Time cumulative: 114.5996744632721 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
          ]
        }
      ],
      "source": [
        "best_rmse_ep, best_mae_ep, best_ndcg_ep = 0, 0, 0\n",
        "best_rmse, best_mae, best_ndcg = float(\"inf\"), float(\"inf\"), 0\n",
        "\n",
        "time_cumulative = 0\n",
        "tic = time()\n",
        "\n",
        "# Pre-Training\n",
        "optimizer = torch.optim.AdamW(complete_model.local_kernel_net.parameters(), lr=0.001)\n",
        "\n",
        "def closure():\n",
        "  optimizer.zero_grad()\n",
        "  x = torch.Tensor(train_r).double().to(device)\n",
        "  m = torch.Tensor(train_m).double().to(device)\n",
        "  complete_model.local_kernel_net.train()\n",
        "  pred, reg = complete_model.local_kernel_net(x)\n",
        "  loss = Loss().to(device)(pred, reg, m, x)\n",
        "  loss.backward()\n",
        "  return loss\n",
        "\n",
        "last_rmse = np.inf\n",
        "counter = 0\n",
        "\n",
        "for i in range(max_epoch_p):\n",
        "  optimizer.step(closure)\n",
        "  complete_model.local_kernel_net.eval()\n",
        "  t = time() - tic\n",
        "  time_cumulative += t\n",
        "\n",
        "  pre, _ = model(torch.Tensor(train_r).double().to(device))\n",
        "\n",
        "  pre = pre.float().cpu().detach().numpy()\n",
        "\n",
        "  error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
        "  test_rmse = np.sqrt(error)\n",
        "\n",
        "  error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "  train_rmse = np.sqrt(error_train)\n",
        "\n",
        "  if last_rmse-train_rmse < tol_p:\n",
        "    counter += 1\n",
        "  else:\n",
        "    counter = 0\n",
        "\n",
        "  last_rmse = train_rmse\n",
        "\n",
        "  if patience_p == counter:\n",
        "    print('.-^-._' * 12)\n",
        "    print('PRE-TRAINING')\n",
        "    print('Epoch:', i+1, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
        "    print('Time:', t, 'seconds')\n",
        "    print('Time cumulative:', time_cumulative, 'seconds')\n",
        "    print('.-^-._' * 12)\n",
        "    break\n",
        "\n",
        "\n",
        "  if i % 50 != 0:\n",
        "    continue\n",
        "  print('.-^-._' * 12)\n",
        "  print('PRE-TRAINING')\n",
        "  print('Epoch:', i, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
        "  print('Time:', t, 'seconds')\n",
        "  print('Time cumulative:', time_cumulative, 'seconds')\n",
        "  print('.-^-._' * 12)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-Tuning\n",
        "\n",
        "train_r_local = np.clip(pre, 1., 5.)\n",
        "\n",
        "optimizer = torch.optim.AdamW(complete_model.parameters(), lr=0.001)\n",
        "\n",
        "def closure():\n",
        "  optimizer.zero_grad()\n",
        "  x = torch.Tensor(train_r).double().to(device)\n",
        "  x_local = torch.Tensor(train_r_local).double().to(device)\n",
        "  m = torch.Tensor(train_m).double().to(device)\n",
        "  complete_model.train()\n",
        "  pred, reg = complete_model(x, x_local)\n",
        "  loss = Loss().to(device)(pred, reg, m, x)\n",
        "  loss.backward()\n",
        "  return loss\n",
        "\n",
        "last_rmse = np.inf\n",
        "counter = 0\n",
        "\n",
        "for i in range(max_epoch_f):\n",
        "  optimizer.step(closure)\n",
        "  complete_model.eval()\n",
        "  t = time() - tic\n",
        "  time_cumulative += t\n",
        "\n",
        "  pre, _ = complete_model(torch.Tensor(train_r).double().to(device), torch.Tensor(train_r_local).double().to(device))\n",
        "\n",
        "  pre = pre.float().cpu().detach().numpy()\n",
        "\n",
        "  error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
        "  test_rmse = np.sqrt(error)\n",
        "\n",
        "  error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "  train_rmse = np.sqrt(error_train)\n",
        "\n",
        "  test_mae = (test_m * np.abs(np.clip(pre, 1., 5.) - test_r)).sum() / test_m.sum()\n",
        "  train_mae = (train_m * np.abs(np.clip(pre, 1., 5.) - train_r)).sum() / train_m.sum()\n",
        "\n",
        "  test_ndcg = call_ndcg(np.clip(pre, 1., 5.), test_r)\n",
        "  train_ndcg = call_ndcg(np.clip(pre, 1., 5.), train_r)\n",
        "\n",
        "  if test_rmse < best_rmse:\n",
        "      best_rmse = test_rmse\n",
        "      best_rmse_ep = i+1\n",
        "\n",
        "  if test_mae < best_mae:\n",
        "      best_mae = test_mae\n",
        "      best_mae_ep = i+1\n",
        "\n",
        "  if best_ndcg < test_ndcg:\n",
        "      best_ndcg = test_ndcg\n",
        "      best_ndcg_ep = i+1\n",
        "\n",
        "  if last_rmse-train_rmse < tol_f:\n",
        "    counter += 1\n",
        "  else:\n",
        "    counter = 0\n",
        "\n",
        "  last_rmse = train_rmse\n",
        "\n",
        "  if patience_f == counter:\n",
        "    print('.-^-._' * 12)\n",
        "    print('FINE-TUNING')\n",
        "    print('Epoch:', i+1, 'test rmse:', test_rmse, 'test mae:', test_mae, 'test ndcg:', test_ndcg)\n",
        "    print('Epoch:', i+1, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
        "    print('Time:', t, 'seconds')\n",
        "    print('Time cumulative:', time_cumulative, 'seconds')\n",
        "    print('.-^-._' * 12)\n",
        "    break\n",
        "\n",
        "\n",
        "  if i % 50 != 0:\n",
        "    continue\n",
        "\n",
        "  print('.-^-._' * 12)\n",
        "  print('FINE-TUNING')\n",
        "  print('Epoch:', i, 'test rmse:', test_rmse, 'test mae:', test_mae, 'test ndcg:', test_ndcg)\n",
        "  print('Epoch:', i, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
        "  print('Time:', t, 'seconds')\n",
        "  print('Time cumulative:', time_cumulative, 'seconds')\n",
        "  print('.-^-._' * 12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6v_tODcweLn",
        "outputId": "04483cd0-4e06-42d5-b1d2-a1edcf845fb7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 0 test rmse: 1.0460932 test mae: 0.83961314 test ndcg: 0.836620168824735\n",
            "Epoch: 0 train rmse: 1.0328672 train mae: 0.8269785 train ndcg: 0.8454367265590808\n",
            "Time: 13.203951835632324 seconds\n",
            "Time cumulative: 127.80362629890442 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 50 test rmse: 0.92251134 test mae: 0.7287125 test ndcg: 0.8899629866935694\n",
            "Epoch: 50 train rmse: 0.8758292 train mae: 0.6930946 train ndcg: 0.9030734556178664\n",
            "Time: 47.55396223068237 seconds\n",
            "Time cumulative: 1666.4289050102234 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 100 test rmse: 0.90295506 test mae: 0.70984524 test ndcg: 0.8962239466122464\n",
            "Epoch: 100 train rmse: 0.8540242 train mae: 0.6734955 train ndcg: 0.9069624508205076\n",
            "Time: 81.44887900352478 seconds\n",
            "Time cumulative: 4910.101551055908 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 150 test rmse: 0.9002911 test mae: 0.70841545 test ndcg: 0.8980184400588871\n",
            "Epoch: 150 train rmse: 0.85315543 train mae: 0.67359024 train ndcg: 0.9074195477193477\n",
            "Time: 115.31503295898438 seconds\n",
            "Time cumulative: 9846.50442647934 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 200 test rmse: 0.9005378 test mae: 0.7104986 test ndcg: 0.897720557900153\n",
            "Epoch: 200 train rmse: 0.8535696 train mae: 0.67599744 train ndcg: 0.9079272520665397\n",
            "Time: 149.0732388496399 seconds\n",
            "Time cumulative: 16473.161125659943 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 250 test rmse: 0.8989076 test mae: 0.7091123 test ndcg: 0.8979236520749866\n",
            "Epoch: 250 train rmse: 0.8483235 train mae: 0.6715163 train ndcg: 0.9086100920263191\n",
            "Time: 182.85737657546997 seconds\n",
            "Time cumulative: 24788.416371822357 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 300 test rmse: 0.897729 test mae: 0.7074537 test ndcg: 0.8976125640499036\n",
            "Epoch: 300 train rmse: 0.8439796 train mae: 0.6678505 train ndcg: 0.9108921038583465\n",
            "Time: 216.72499179840088 seconds\n",
            "Time cumulative: 34794.87114238739 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 350 test rmse: 0.897261 test mae: 0.7064034 test ndcg: 0.896701702802436\n",
            "Epoch: 350 train rmse: 0.84151316 train mae: 0.66527116 train ndcg: 0.9112071571183367\n",
            "Time: 250.48931312561035 seconds\n",
            "Time cumulative: 46491.84305238724 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 400 test rmse: 0.8969079 test mae: 0.7061785 test ndcg: 0.8967718653447232\n",
            "Epoch: 400 train rmse: 0.8399842 train mae: 0.663903 train ndcg: 0.9118925293888019\n",
            "Time: 284.3279137611389 seconds\n",
            "Time cumulative: 59878.77181863785 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 450 test rmse: 0.89742863 test mae: 0.7064704 test ndcg: 0.8970977061425852\n",
            "Epoch: 450 train rmse: 0.838865 train mae: 0.66270715 train ndcg: 0.9120410011971886\n",
            "Time: 318.1552360057831 seconds\n",
            "Time cumulative: 74958.02976131439 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 500 test rmse: 0.8970469 test mae: 0.70565045 test ndcg: 0.8965650121308296\n",
            "Epoch: 500 train rmse: 0.8384719 train mae: 0.662364 train ndcg: 0.9125711117209251\n",
            "Time: 352.0544764995575 seconds\n",
            "Time cumulative: 91730.42343378067 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 550 test rmse: 0.89720875 test mae: 0.70673364 test ndcg: 0.8972412240172347\n",
            "Epoch: 550 train rmse: 0.83814126 train mae: 0.66297656 train ndcg: 0.9125856308739329\n",
            "Time: 385.91097378730774 seconds\n",
            "Time cumulative: 110197.24574947357 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 600 test rmse: 0.8968042 test mae: 0.70629174 test ndcg: 0.8962726111695318\n",
            "Epoch: 600 train rmse: 0.8372391 train mae: 0.6620127 train ndcg: 0.9132698834422784\n",
            "Time: 419.6881124973297 seconds\n",
            "Time cumulative: 130354.3627064228 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 650 test rmse: 0.89701295 test mae: 0.70625913 test ndcg: 0.8957205509157279\n",
            "Epoch: 650 train rmse: 0.83650327 train mae: 0.6615652 train ndcg: 0.9133666711776909\n",
            "Time: 453.4969370365143 seconds\n",
            "Time cumulative: 152201.36920070648 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 700 test rmse: 0.8979104 test mae: 0.70806354 test ndcg: 0.8968477331284763\n",
            "Epoch: 700 train rmse: 0.83638763 train mae: 0.66236645 train ndcg: 0.9131135248865462\n",
            "Time: 487.3077175617218 seconds\n",
            "Time cumulative: 175738.4918870926 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 750 test rmse: 0.8973577 test mae: 0.7071753 test ndcg: 0.8979194209454419\n",
            "Epoch: 750 train rmse: 0.83604985 train mae: 0.66170216 train ndcg: 0.9133814384955314\n",
            "Time: 521.1907305717468 seconds\n",
            "Time cumulative: 200969.2776772976 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 800 test rmse: 0.8969925 test mae: 0.7059343 test ndcg: 0.8970648758254398\n",
            "Epoch: 800 train rmse: 0.8352188 train mae: 0.66046196 train ndcg: 0.9139897215156041\n",
            "Time: 555.1359848976135 seconds\n",
            "Time cumulative: 227893.60344099998 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 850 test rmse: 0.896124 test mae: 0.7045594 test ndcg: 0.896681411862833\n",
            "Epoch: 850 train rmse: 0.83431745 train mae: 0.6588307 train ndcg: 0.9145914399665395\n",
            "Time: 589.1122152805328 seconds\n",
            "Time cumulative: 256517.6897597313 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 900 test rmse: 0.89588517 test mae: 0.704032 test ndcg: 0.8967759681002034\n",
            "Epoch: 900 train rmse: 0.83297366 train mae: 0.6574216 train ndcg: 0.9140593735419746\n",
            "Time: 622.9651112556458 seconds\n",
            "Time cumulative: 286836.79408741 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 950 test rmse: 0.89631915 test mae: 0.70526844 test ndcg: 0.8970316760154211\n",
            "Epoch: 950 train rmse: 0.83204854 train mae: 0.6580144 train ndcg: 0.9144316055946458\n",
            "Time: 656.9254522323608 seconds\n",
            "Time cumulative: 318851.5760936737 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTi_PdXJqTjh",
        "outputId": "f955345e-992c-42dc-a2ea-7ebba9190360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 969  best rmse: 0.89548534\n",
            "Epoch: 924  best mae: 0.70280766\n",
            "Epoch: 238  best ndcg: 0.8991677653204664\n"
          ]
        }
      ],
      "source": [
        "# Final result\n",
        "print('Epoch:', best_rmse_ep, ' best rmse:', best_rmse)\n",
        "print('Epoch:', best_mae_ep, ' best mae:', best_mae)\n",
        "print('Epoch:', best_ndcg_ep, ' best ndcg:', best_ndcg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정확도(RMSE)를 계산하는 함수\n",
        "def RMSE2(y_true, y_pred):\n",
        "    return np.sqrt(np.mean((np.array(y_true) - np.array(y_pred))**2))"
      ],
      "metadata": {
        "id": "PzjdeaIrChti"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_ids = ratings_test.user_idx.values\n",
        "item_ids = ratings_test.item_idx.values"
      ],
      "metadata": {
        "id": "V_2WHkXACoQ5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = pre[item_ids, user_ids]\n",
        "y_true = np.array(ratings_test.rating)\n",
        "\n",
        "RMSE2(y_true, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbwT8mnoup4i",
        "outputId": "9387ecee-64f3-4303-ef8b-239721455ec9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2460722221485923"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}